{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df1f5070-8464-43b2-bba2-bef4aec25b45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if files are visible\n",
    "display(dbutils.fs.ls(\"/Volumes/workspace/default/cscie103_final_project/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c206ca8-02ce-48d2-9bf1-ae212ddb35d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bronze Layer Ingestion\n",
    "- Ingests raw CSV files from Volumes into Bronze Delta Tables.\n",
    "- Uses \"Overwrite\" mode to handle full daily refreshes \n",
    "  and \"overwriteSchema\" to handle structural changes in source data.\n",
    "- Outputting as Delta allows downstream \n",
    "- Silver layer to use .readStream\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "def ingest_bronze(table_name, file_name):\n",
    "    \"\"\"\n",
    "    Reads csv file from the project volume and overwrites the target Bronze Delta table.\n",
    "    \"\"\"\n",
    "    \n",
    "    source_path = f\"/Volumes/workspace/default/cscie103_final_project/{file_name}\"\n",
    "    target_table_name = f\"bronze_{table_name}\"\n",
    "    \n",
    "    print(f\"Processing {table_name}\")\n",
    "    \n",
    "    # Read the CSV (Bronze, schema is inferred)\n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(source_path)\n",
    "    \n",
    "    # Adding timestamp to track when this specific batch was processed.\n",
    "    # Adding input_file to track which file this row came from.\n",
    "    df_enriched = df.select(\"*\", \"_metadata.file_path\") \\\n",
    "        .withColumnRenamed(\"file_path\", \"_source_file\") \\\n",
    "        .withColumn(\"_ingestion_time\", current_timestamp())\n",
    "    \n",
    "    # Write to Delta table\n",
    "    # model(\"overwrite\"), overwriteSchema - ensures idempotency/robustness\n",
    "    df_enriched.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(target_table_name)\n",
    "    \n",
    "    print(f\"Success: refreshed table: {target_table_name}\")\n",
    "\n",
    "# Ingest all files required for pipeline\n",
    "ingest_bronze(\"client\", \"client.csv\")\n",
    "ingest_bronze(\"train\", \"train.csv\")\n",
    "ingest_bronze(\"gas_prices\", \"gas_prices.csv\")\n",
    "ingest_bronze(\"electricity_prices\", \"electricity_prices.csv\")\n",
    "ingest_bronze(\"weather_hist\", \"historical_weather.csv\")\n",
    "ingest_bronze(\"weather_forecast\", \"forecast_weather.csv\")\n",
    "ingest_bronze(\"weather_mapping\", \"weather_station_to_county_mapping.csv\") \n",
    "\n",
    "print(\"\\nBronze Layer Ingestion Complete\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingest_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
