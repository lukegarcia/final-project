{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0676f719-8769-4ff0-a0e4-415ac2170563",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Gold Layer\nCreating table...\nTable created.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "GOLD LAYER\n",
    "- aggregate for daily reporting and upserts\n",
    "\"\"\"\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col, sum, avg, to_date\n",
    "\n",
    "def create_gold():\n",
    "    print(\"Building Gold Layer\")\n",
    "\n",
    "\n",
    "    bronze_df = (\n",
    "        spark.read.table(\"cscie103_catalog_final.bronze.train\")\n",
    "        .withColumn(\"date\", to_date(\"datetime\"))\n",
    "    )\n",
    "\n",
    "    # Consumption aggregated by date + county + is_business + product_type\n",
    "    consumption_df = (\n",
    "        bronze_df\n",
    "        .filter(col(\"is_consumption\") == 1)\n",
    "        .groupBy(\"date\", \"county\", \"is_business\", \"product_type\")\n",
    "        .agg(\n",
    "            sum(\"target\").alias(\"total_energy\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 2Production aggregated by date + county + is_business + product_type\n",
    "    production_df = (\n",
    "        bronze_df\n",
    "        .filter(col(\"is_consumption\") == 0)\n",
    "        .groupBy(\"date\", \"county\", \"is_business\", \"product_type\")\n",
    "        .agg(\n",
    "            sum(\"target\").alias(\"total_energy_production\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Combine consumption + production\n",
    "    energy_df = (\n",
    "        consumption_df\n",
    "        .join(\n",
    "            production_df,\n",
    "            on=[\"date\", \"county\", \"is_business\", \"product_type\"],\n",
    "            how=\"full_outer\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Weather metrics per date + county (from county_weather_4hours_vw)\n",
    "    weather_df = (\n",
    "        spark.read.table(\"cscie103_catalog_final.gold.county_weather_4hours_vw\")\n",
    "        .groupBy(\"wh_observ_date\", \"county_id\")\n",
    "        .agg(\n",
    "            avg(\"wf_temperature\").alias(\"avg_temp\"),\n",
    "            avg(\"wf_direct_solar_radiation\").alias(\"avg_radiation\")\n",
    "        )\n",
    "        .withColumnRenamed(\"wh_observ_date\", \"date\")\n",
    "        .withColumnRenamed(\"county_id\", \"county\")\n",
    "    )\n",
    "\n",
    "    # Join energy + weather\n",
    "    report_df = energy_df.join(weather_df, on=[\"date\", \"county\"], how=\"left\")\n",
    "\n",
    "    # 5County name mapping\n",
    "    county_mapping_df = (\n",
    "        spark.read.table(\"cscie103_catalog_final.silver.county_mapping\")\n",
    "        .select(\n",
    "            col(\"county_id\").alias(\"county\"),\n",
    "            col(\"county_name\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # County geo (1:1 county_id → lat/long)\n",
    "    county_geo_df = (\n",
    "        spark.read.table(\"cscie103_catalog_final.silver.county_geo\")\n",
    "        .select(\n",
    "            col(\"county_id\").alias(\"county\"),\n",
    "            col(\"county_latitude\").alias(\"latitude\"),\n",
    "            col(\"county_longitude\").alias(\"longitude\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Product mapping: product_type code → human-readable name\n",
    "    product_mapping_df = (\n",
    "        spark.read.table(\"cscie103_catalog_final.silver.product_mapping\")\n",
    "        .select(\n",
    "            col(\"product_id\").alias(\"product_type\"),        # join key\n",
    "            col(\"product_type\").alias(\"product_type_name\")  # label\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Join in county attributes + product info\n",
    "    enriched_df = (\n",
    "        report_df\n",
    "        .join(county_mapping_df, on=\"county\", how=\"left\")\n",
    "        .join(county_geo_df, on=\"county\", how=\"left\")\n",
    "        .join(product_mapping_df, on=\"product_type\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    # Convert is_business: 1 → True, otherwise False\n",
    "    enriched_df = enriched_df.withColumn(\n",
    "        \"is_business\",\n",
    "        (col(\"is_business\") == 1)\n",
    "    )\n",
    "\n",
    "    # Final selected columns / ordering\n",
    "    final_df = enriched_df.select(\n",
    "        \"date\",\n",
    "        \"county\",\n",
    "        \"county_name\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"is_business\",              # boolean\n",
    "        \"product_type\",             # 0 / 1 / 2 / 3\n",
    "        \"product_type_name\",        # from product_mapping\n",
    "        \"total_energy\",\n",
    "        \"total_energy_production\",\n",
    "        \"avg_temp\",\n",
    "        \"avg_radiation\"\n",
    "    )\n",
    "\n",
    "    # Target table\n",
    "    target_table_name = \"cscie103_catalog_final.gold.daily_energy_report\"\n",
    "\n",
    "    # Create table if it does not exist\n",
    "    if not spark.catalog.tableExists(target_table_name):\n",
    "        print(\"Creating table...\")\n",
    "        final_df.write.format(\"delta\").saveAsTable(target_table_name)\n",
    "        print(\"Table created.\")\n",
    "        return\n",
    "\n",
    "    # Upsert / merge into existing Delta table\n",
    "    deltaTable = DeltaTable.forName(spark, target_table_name)\n",
    "\n",
    "    (\n",
    "        deltaTable.alias(\"t\")\n",
    "        .merge(\n",
    "            final_df.alias(\"s\"),\n",
    "            \"\"\"\n",
    "            t.date = s.date\n",
    "            AND t.county = s.county\n",
    "            AND t.is_business = s.is_business\n",
    "            AND t.product_type = s.product_type\n",
    "            \"\"\"\n",
    "        )\n",
    "        .whenMatchedUpdateAll()\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "    print(f\"Merge/Upsert complete for {target_table_name}\")\n",
    "\n",
    "create_gold()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "updated_gold_layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}